{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Principles of WGCNA (Weighted Correlation Network Analysis)\n",
    "\n",
    "## Objective: Based on the gene expression matrix, identify groups of genes (modules) with similar expression patterns and relate them to external trait data (e.g., disease, treatment, and time).\n",
    "\n",
    "## 1. Input Gene Expression Matrix\n",
    "\n",
    "Let the gene expression matrix be:\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{n \\times p}\n",
    "$$\n",
    "\n",
    "- $n$: number of samples\n",
    "- $p$: number of genes\n",
    "- The $i$-th row $x_i = (x_{i1}, x_{i2}, \\ldots, x_{ip})$ is the expression vector of sample $i$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Similarity Matrix\n",
    "\n",
    "Define the Pearson correlation coefficient between any two genes $i$ and $j$:\n",
    "\n",
    "$$\n",
    "s_{ij} = \\text{cor}(x_{\\cdot i}, x_{\\cdot j}) = \\frac{\\sum_{k=1}^n (x_{ki} - \\bar{x}_{\\cdot i})(x_{kj} - \\bar{x}_{\\cdot j})}{\\sqrt{\\sum_{k=1}^n (x_{ki} - \\bar{x}_{\\cdot i})^2} \\sqrt{\\sum_{k=1}^n (x_{kj} - \\bar{x}_{\\cdot j})^2}}\n",
    "$$\n",
    "\n",
    "where $\\bar{x}_{\\cdot i}$ is the sample mean of gene $i$.\n",
    "\n",
    "- The similarity matrix $S = (s_{ij})$ is symmetric, and $s_{ii} = 1$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Adjacency Matrix\n",
    "\n",
    "Define the adjacency matrix $A = (a_{ij})$:\n",
    "\n",
    "$$\n",
    "a_{ij} = |s_{ij}|^\\beta\n",
    "$$\n",
    "\n",
    "- $\\beta > 1$ is the soft-thresholding power\n",
    "- Typically, $\\beta$ is chosen to make the network approximate a scale-free topology\n",
    "\n",
    "### Scale-free Topology\n",
    "\n",
    "In an undirected graph $G = (V, E)$:\n",
    "\n",
    "- $V$ is the set of nodes, with $|V| = N$\n",
    "- $E$ is the set of edges\n",
    "- Each node $v \\in V$ has a degree $k_v$, representing the number of connections\n",
    "\n",
    "Define the degree distribution $P(k)$ as:\n",
    "\n",
    "$$\n",
    "P(k) = \\frac{\\text{number of nodes with degree } k}{N}\n",
    "$$\n",
    "\n",
    "A network is said to have **scale-free topology** if its degree distribution satisfies the following power-law:\n",
    "\n",
    "$$\n",
    "P(k) \\propto k^{-\\gamma}, \\quad k \\geq k_{\\text{min}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\gamma > 1$ is the power-law exponent, usually in the range $(2,3)$\n",
    "- $k_{\\text{min}}$ is the minimum degree for which the power-law holds\n",
    "\n",
    "In other words, there exists a constant $C > 0$ such that for all $k \\geq k_{\\text{min}}$:\n",
    "\n",
    "$$\n",
    "P(k) = C k^{-\\gamma}\n",
    "$$\n",
    "\n",
    "#### Log-log Transformation\n",
    "\n",
    "Taking the logarithm of both sides:\n",
    "\n",
    "$$\n",
    "\\log P(k) = -\\gamma \\log k + \\log C\n",
    "$$\n",
    "\n",
    "Thus, in log-log coordinates, $P(k)$ versus $k$ appears as a straight line with slope $-\\gamma$.\n",
    "\n",
    "#### Properties of Power-law Distributions\n",
    "\n",
    "Power-law distributions have the following mathematical properties:\n",
    "\n",
    "1. **Scale-free**: No characteristic scale; the degree can vary across several orders of magnitude. For power-law distributions, the expectation and variance may diverge depending on $\\gamma$:\n",
    "   - When $2 < \\gamma \\leq 3$, the expectation is finite but variance is infinite\n",
    "   - When $1 < \\gamma \\leq 2$, both expectation and variance diverge\n",
    "\n",
    "2. **Heavy-tailed**: High-degree nodes (hub nodes) are rare but occur more frequently than in exponential or normal distributions\n",
    "\n",
    "#### Normalization Constant of Degree Distribution\n",
    "\n",
    "To make $P(k)$ a valid probability distribution, normalization is required:\n",
    "\n",
    "$$\n",
    "\\sum_{k = k_{\\text{min}}}^{\\infty} P(k) = 1\n",
    "$$\n",
    "\n",
    "Substitute the power-law form:\n",
    "\n",
    "$$\n",
    "C \\sum_{k = k_{\\text{min}}}^{\\infty} k^{-\\gamma} = 1\n",
    "$$\n",
    "\n",
    "So the normalization constant $C$ is:\n",
    "\n",
    "$$\n",
    "C = \\left( \\sum_{k = k_{\\text{min}}}^{\\infty} k^{-\\gamma} \\right)^{-1}\n",
    "$$\n",
    "\n",
    "In the continuous approximation (with $k$ as a continuous variable), normalization can be approximated by integration:\n",
    "\n",
    "$$\n",
    "\\int_{k_{\\text{min}}}^{\\infty} C k^{-\\gamma} \\, dk = 1\n",
    "$$\n",
    "\n",
    "Solving gives:\n",
    "\n",
    "$$\n",
    "C = (\\gamma - 1) k_{\\text{min}}^{\\gamma - 1}\n",
    "$$\n",
    "\n",
    "#### Summary Definition of a Scale-free Network\n",
    "\n",
    "**Mathematically defined**:\n",
    "\n",
    "A graph $G = (V, E)$ satisfies:\n",
    "\n",
    "- There exists $\\gamma > 1$ such that the degree distribution follows $P(k) \\sim k^{-\\gamma}$\n",
    "- For $k \\geq k_{\\text{min}}$\n",
    "- And typically $\\gamma$ lies within $[2,3]$\n",
    "\n",
    "Then $G$ is called a **scale-free network**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Topological Overlap Matrix (TOM)\n",
    "\n",
    "Define the number of shared neighbors:\n",
    "\n",
    "$$\n",
    "l_{ij} = \\sum_{u=1}^{p} a_{iu} a_{ju}\n",
    "$$\n",
    "\n",
    "- $l_{ij}$ is the total strength of shared neighbors between nodes $i$ and $j$\n",
    "\n",
    "Degree of node $i$:\n",
    "\n",
    "$$\n",
    "k_i = \\sum_{u=1}^{p} a_{iu}\n",
    "$$\n",
    "\n",
    "- $k_i$ is the total weight of edges connecting node $i$ to others\n",
    "\n",
    "### Elements of the Topological Overlap Matrix:\n",
    "\n",
    "$$\n",
    "\\text{TOM}_{ij} = \\frac{l_{ij} + a_{ij}}{\\min(k_i, k_j) + 1 - a_{ij}}\n",
    "$$\n",
    "\n",
    "- Numerator: $l_{ij} + a_{ij}$, i.e., shared neighbor strength plus direct connection\n",
    "- Denominator: $\\min(k_i, k_j) + 1 - a_{ij}$, a normalization factor to avoid bias from high-degree hubs\n",
    "\n",
    "### Properties of the Topological Overlap Matrix:\n",
    "\n",
    "- $\\text{TOM}_{ii} = 1$: maximum similarity with itself\n",
    "- $0 \\leq \\text{TOM}_{ij} \\leq 1$: standardized between 0 and 1\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Higher TOM** (close to 1): nodes $i$ and $j$ share more neighbors and may be directly connected\n",
    "- **Lower TOM** (close to 0): nodes $i$ and $j$ share few or no common neighbors, more distant relationship\n",
    "\n",
    "### Concrete Example of TOM Calculation\n",
    "\n",
    "#### Suppose we have a small adjacency matrix $A$ with 4 nodes ($p = 4$):\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "1 & 0 & 1 & 1 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Explanation:\n",
    "- Node 1 is connected to 2 and 3 (weight 1)\n",
    "- Node 2 is connected to 1, 3, and 4 (weight 1)\n",
    "- Node 3 is connected to 1 and 2\n",
    "- Node 4 is connected to 2\n",
    "\n",
    "#### Step 1: Compute Node Degrees $k_i$\n",
    "\n",
    "- $k_1 = a_{12} + a_{13} = 1 + 1 = 2$\n",
    "- $k_2 = a_{21} + a_{23} + a_{24} = 1 + 1 + 1 = 3$\n",
    "- $k_3 = a_{31} + a_{32} = 1 + 1 = 2$\n",
    "- $k_4 = a_{42} = 1$\n",
    "\n",
    "#### Step 2: Compute Shared Neighbors $l_{ij}$\n",
    "\n",
    "$$\n",
    "l_{ij} = \\sum_{u=1}^{p} a_{iu} a_{ju}\n",
    "$$\n",
    "\n",
    "- $l_{12} = (0)(1) + (1)(0) + (1)(1) + (0)(1) = 1$\n",
    "- $l_{13} = (0)(1) + (1)(1) + (1)(0) + (0)(0) = 1$\n",
    "- $l_{14} = (0)(0) + (1)(1) + (1)(0) + (0)(0) = 1$\n",
    "- $l_{23} = (1)(1) + (0)(1) + (1)(0) + (1)(0) = 1$\n",
    "- $l_{24} = (1)(0) + (0)(1) + (1)(0) + (1)(0) = 0$\n",
    "- $l_{34} = (1)(0) + (1)(1) + (0)(0) + (0)(0) = 1$\n",
    "\n",
    "#### Step 3: Compute $\\text{TOM}_{ij}$\n",
    "\n",
    "$$\n",
    "\\text{TOM}_{ij} = \\frac{l_{ij} + a_{ij}}{\\min(k_i, k_j) + 1 - a_{ij}}\n",
    "$$\n",
    "\n",
    "- $\\text{TOM}_{12} = \\frac{1 + 1}{2 + 1 - 1} = \\frac{2}{2} = 1$\n",
    "- $\\text{TOM}_{13} = \\frac{1 + 1}{2 + 1 - 1} = \\frac{2}{2} = 1$\n",
    "- $\\text{TOM}_{14} = \\frac{1 + 0}{1 + 1 - 0} = \\frac{1}{2} = 0.5$\n",
    "- $\\text{TOM}_{23} = \\frac{1 + 1}{2 + 1 - 1} = \\frac{2}{2} = 1$\n",
    "- $\\text{TOM}_{24} = \\frac{0 + 1}{1 + 1 - 1} = \\frac{1}{1} = 1$\n",
    "- $\\text{TOM}_{34} = \\frac{1 + 0}{1 + 1 - 0} = \\frac{1}{2} = 0.5$\n",
    "\n",
    "#### Final TOM Matrix:\n",
    "\n",
    "$$\n",
    "\\text{TOM} = \\begin{pmatrix}\n",
    "1 & 1 & 1 & 0.5 \\\\\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 0.5 \\\\\n",
    "0.5 & 1 & 0.5 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 5. Gene Module Detection\n",
    "\n",
    "Define the distance matrix:\n",
    "\n",
    "$$\n",
    "d_{ij} = 1 - \\text{TOM}_{ij}\n",
    "$$\n",
    "\n",
    "- Use hierarchical clustering\n",
    "- Apply Dynamic Tree Cut to automatically cut the clustering tree\n",
    "- Each module $M_k$ is a set of genes\n",
    "\n",
    "### 5.1 Define the Topological Overlap Matrix (TOM)\n",
    "\n",
    "Given the adjacency matrix $A = (a_{ij})$, the node degree is:\n",
    "\n",
    "$$\n",
    "k_i = \\sum_{u=1}^p a_{iu}\n",
    "$$\n",
    "\n",
    "The number of shared neighbors is:\n",
    "\n",
    "$$\n",
    "l_{ij} = \\sum_{u=1}^p a_{iu} a_{ju}\n",
    "$$\n",
    "\n",
    "The element of the topological overlap matrix is defined as:\n",
    "\n",
    "$$\n",
    "\\text{TOM}_{ij} = \\frac{l_{ij} + a_{ij}}{\\min(k_i, k_j) + 1 - a_{ij}}\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- $\\text{TOM}_{ii} = 1$\n",
    "- $0 \\leq \\text{TOM}_{ij} \\leq 1$\n",
    "\n",
    "### 5.2 Define the Distance Matrix $D$\n",
    "\n",
    "Based on the topological overlap matrix, define the distance matrix $D = (d_{ij})$:\n",
    "\n",
    "$$\n",
    "d_{ij} = 1 - \\text{TOM}_{ij}\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- $d_{ii} = 0$\n",
    "- $0 \\leq d_{ij} \\leq 1$\n",
    "\n",
    "### 5.3 Hierarchical Clustering\n",
    "\n",
    "Perform hierarchical clustering on the distance matrix $D$ to build a dendrogram $\\mathcal{T}$:\n",
    "\n",
    "- Initial state: each gene is an independent cluster\n",
    "- Merging strategy: minimize the inter-cluster distance according to the linkage function $\\mathcal{L}(C_1, C_2)$\n",
    "- Typically use average linkage:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(C_1, C_2) = \\frac{1}{|C_1||C_2|} \\sum_{i \\in C_1} \\sum_{j \\in C_2} d_{ij}\n",
    "$$\n",
    "\n",
    "### 5.4 Dynamic Tree Cut\n",
    "\n",
    "Apply the Dynamic Tree Cut algorithm to the dendrogram $\\mathcal{T}$ to identify modules:\n",
    "\n",
    "- Define a subtree $M_k \\subseteq V$ such that:\n",
    "  - Internal similarity is high (TOM high, $d$ low)\n",
    "  - Heterogeneity between subtrees is high (TOM low, $d$ high)\n",
    "- Dynamic Tree Cut automatically determines the number and size of modules\n",
    "\n",
    "Finally, obtain a set of modules:\n",
    "\n",
    "$$\n",
    "\\{ M_1, M_2, \\ldots, M_K \\}\n",
    "$$\n",
    "\n",
    "where $M_k \\subseteq V$, and:\n",
    "\n",
    "$$\n",
    "\\bigcup_{k=1}^{K} M_k = V\n",
    "$$\n",
    "\n",
    "(all genes are completely assigned to at least one module without overlap)\n",
    "\n",
    "### Mathematical Workflow Summary\n",
    "\n",
    "1. Input: adjacency matrix $A$\n",
    "2. Calculate the topological overlap matrix $\\text{TOM}$\n",
    "3. Calculate the distance matrix $d_{ij} = 1 - \\text{TOM}_{ij}$\n",
    "4. Build the hierarchical clustering tree $\\mathcal{T}$ based on $D$\n",
    "5. Apply dynamic tree cutting to obtain the module partition $\\{ M_k \\}$\n",
    "\n",
    "### âœ¨ Additional Mathematical Properties\n",
    "\n",
    "- During clustering, the dendrogram $\\mathcal{T}$ guarantees the ultrametric property:\n",
    "\n",
    "  For any three nodes $i, j, k$:\n",
    "\n",
    "  $$\n",
    "  d_{ij} \\leq \\max(d_{ik}, d_{jk})\n",
    "  $$\n",
    "\n",
    "- Dynamic Tree Cut allows modules of unequal sizes, avoiding the limitation of fixed-radius cuts\n",
    "\n",
    "### First Part: Why Does the Clustering Tree $\\mathcal{T}$ Satisfy the Ultrametric Property?\n",
    "\n",
    "Recall the **mathematical definition** of ultrametric:\n",
    "\n",
    "For any three points $i, j, k$:\n",
    "\n",
    "$$\n",
    "d_{ij} \\leq \\max(d_{ik}, d_{jk})\n",
    "$$\n",
    "\n",
    "which is stricter than the standard triangle inequality ($d_{ij} \\leq d_{ik} + d_{kj}$).\n",
    "\n",
    "**Hierarchical clustering** naturally ensures the ultrametric property because:\n",
    "- Hierarchical clustering merges the two closest clusters step-by-step\n",
    "- Each merge updates the distances between all nodes inside the new cluster\n",
    "- The distance between any two nodes is defined by the height at which their clusters merged\n",
    "\n",
    "In **average linkage clustering**, for example, the distance between merged clusters is the average distance between all elements.\n",
    "\n",
    "Thus, in a dendrogram:\n",
    "- The distance between any two nodes is determined by the height of their lowest common ancestor (LCA)\n",
    "\n",
    "Thus, for any three points $i, j, k$:\n",
    "- $d_{ij}$ is the height of the LCA of $i$ and $j$\n",
    "- $d_{ik}$ is the height of the LCA of $i$ and $k$\n",
    "- $d_{jk}$ is the height of the LCA of $j$ and $k$\n",
    "\n",
    "Since common ancestors can only be higher (not lower), it follows that:\n",
    "\n",
    "$$\n",
    "d_{ij} \\leq \\max(d_{ik}, d_{jk})\n",
    "$$\n",
    "\n",
    "**Ultrametricity holds naturally.**\n",
    "\n",
    "âœ… **Core summary in one sentence**:\n",
    "\n",
    "> **In a tree, the distance between two nodes is determined by the height of their lowest common ancestor (LCA), naturally satisfying the ultrametric property!**\n",
    "\n",
    "\n",
    "### Second Part: Why Does Dynamic Tree Cut Allow Modules of Unequal Sizes?\n",
    "\n",
    "**What is the traditional tree-cutting method?**\n",
    "\n",
    "- A fixed cutting height is set (e.g., $h = 0.25$), and all branches above that height are cut.\n",
    "- This results in modules of very uniform sizes because all are cut using the same global threshold.\n",
    "\n",
    "âš¡ **But the real biological gene networks are not uniform!**\n",
    "- Some modules are very tight (low internal distance) and can be cut early.\n",
    "- Some modules are more loose (higher internal distance), and need to be cut at a higher level.\n",
    "- Different modules naturally have different densities!\n",
    "\n",
    "**The mathematical core of Dynamic Tree Cut:**\n",
    "\n",
    "It determines the cutting points adaptively based on **local tree structure**, rather than using a single global height.\n",
    "\n",
    "- Small modules: if a subtree is tightly connected, Dynamic Tree Cut will cut it at a lower height.\n",
    "- Large modules: if a subtree is loose, Dynamic Tree Cut will allow it to merge further until internal consistency is reached.\n",
    "\n",
    "In other words:\n",
    "- Dynamic Tree Cut allows the **cutting thresholds to vary dynamically** between modules.\n",
    "- It does **not enforce uniform module sizes**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Module Eigengenes (ME)\n",
    "\n",
    "### 6.1 Basic Setup\n",
    "\n",
    "Given a module $M_k$:\n",
    "\n",
    "- It contains $|M_k|$ genes.\n",
    "- The corresponding gene expression matrix is:\n",
    "\n",
    "$$\n",
    "X_k \\in \\mathbb{R}^{n \\times |M_k|}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ is the number of samples (i.e., each row is a sample)\n",
    "- $|M_k|$ is the number of genes in the module (i.e., each column is gene expression)\n",
    "\n",
    "### 6.2 Definition of Module Eigengene\n",
    "\n",
    "The module eigengene $\\text{ME}_k \\in \\mathbb{R}^n$ is defined as the first principal component of $X_k$:\n",
    "\n",
    "$$\n",
    "\\text{ME}_k = X_k v_1\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $v_1 \\in \\mathbb{R}^{|M_k|}$ is the eigenvector corresponding to the largest eigenvalue of the covariance matrix of $X_k$\n",
    "\n",
    "More formally:\n",
    "\n",
    "- Covariance matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{n-1} X_k^\\top X_k \\in \\mathbb{R}^{|M_k| \\times |M_k|}\n",
    "$$\n",
    "\n",
    "- Solve the eigenvalue problem:\n",
    "\n",
    "$$\n",
    "\\Sigma_k v_1 = \\lambda_1 v_1\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\lambda_1$ is the largest eigenvalue ($\\lambda_1 \\geq \\lambda_2 \\geq \\dots$)\n",
    "- $v_1$ is the corresponding unit eigenvector ($\\|v_1\\| = 1$)\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{ME}_k = X_k v_1\n",
    "$$\n",
    "\n",
    "### 6.3 Why Is It Defined This Way?\n",
    "\n",
    "- $v_1$ gives the **main direction of variation** in gene expression within the module.\n",
    "- Projecting onto $v_1$ gives $\\text{ME}_k$, which is the coordinate of each sample along that direction.\n",
    "- $\\text{ME}_k$ represents the **dominant expression trend** of the whole module across samples.\n",
    "\n",
    "In plain terms:\n",
    "\n",
    "> **Too many genes in a module? No worriesâ€”compress them into a single vector that best captures their overall trend: the ME.**\n",
    "\n",
    "### Optimization Interpretation\n",
    "\n",
    "The module eigengene $\\text{ME}_k$ can also be understood via an optimization problem:\n",
    "\n",
    "$$\n",
    "v_1 = \\arg\\max_{\\|v\\|=1} \\operatorname{Var}(X_k v)\n",
    "$$\n",
    "\n",
    "i.e., find the projection direction that gives the largest variance.\n",
    "\n",
    "### ðŸ“š Pure Math Derivation: Module Eigengene\n",
    "\n",
    "#### 1. Problem Setup\n",
    "\n",
    "Assume the centered module expression matrix is:\n",
    "\n",
    "$$\n",
    "X_k \\in \\mathbb{R}^{n \\times p}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "X_k = \\begin{pmatrix}\n",
    "2 & 3 & 5 \\\\\n",
    "4 & 6 & 8 \\\\\n",
    "1 & 1 & 2 \\\\\n",
    "5 & 7 & 10 \\\\\n",
    "3 & 4 & 6 \\\\\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{5 \\times 3}\n",
    "$$\n",
    "\n",
    "#### 2. Projection and Variance\n",
    "\n",
    "Project onto a direction $v \\in \\mathbb{R}^p$, yielding:\n",
    "\n",
    "$$\n",
    "z = X_k v\n",
    "$$\n",
    "\n",
    "Then the sample variance of $z$ is:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(z) = \\frac{1}{n - 1} z^\\top z\n",
    "$$\n",
    "\n",
    "Substituting $z = X_k v$ gives:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(z) = \\frac{1}{n - 1} v^\\top X_k^\\top X_k v\n",
    "$$\n",
    "\n",
    "#### 3. Covariance Matrix\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{n - 1} X_k^\\top X_k\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(z) = v^\\top \\Sigma_k v\n",
    "$$\n",
    "\n",
    "#### 4. Optimization Problem\n",
    "\n",
    "Find the direction of maximum projection variance:\n",
    "\n",
    "$$\n",
    "v_1 = \\arg\\max_{\\|v\\| = 1} v^\\top \\Sigma_k v\n",
    "$$\n",
    "\n",
    "#### 5. Analytical Solution\n",
    "\n",
    "By linear algebra, the optimal solution $v_1$ is the unit eigenvector corresponding to the largest eigenvalue of $\\Sigma_k$.\n",
    "\n",
    "#### 6. Final Definition of Module Eigengene\n",
    "\n",
    "$$\n",
    "\\text{ME}_k = X_k v_1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Mathematical Example of Module Eigengene (ME)\n",
    "\n",
    "#### Assume a Small Module Expression Matrix $X_k$\n",
    "\n",
    "Suppose module $M_k$ has 3 genes and 5 samples, with expression matrix:\n",
    "\n",
    "$$\n",
    "X_k = \\begin{pmatrix}\n",
    "2 & 3 & 5 \\\\\n",
    "4 & 6 & 8 \\\\\n",
    "1 & 1 & 2 \\\\\n",
    "5 & 7 & 10 \\\\\n",
    "3 & 4 & 6 \\\\\n",
    "\\end{pmatrix}\n",
    "\\in \\mathbb{R}^{5 \\times 3}\n",
    "$$\n",
    "\n",
    "- Each row represents a sample\n",
    "- Each column represents a gene\n",
    "\n",
    "#### Step 1: Mean Centering\n",
    "\n",
    "PCA requires subtracting the mean of each column to center the data.\n",
    "\n",
    "Calculate the mean of each column:\n",
    "\n",
    "- Gene 1 mean:\n",
    "\n",
    "$$\n",
    "\\bar{x}_1 = \\frac{2 + 4 + 1 + 5 + 3}{5} = 3\n",
    "$$\n",
    "\n",
    "- Gene 2 mean:\n",
    "\n",
    "$$\n",
    "\\bar{x}_2 = \\frac{3 + 6 + 1 + 7 + 4}{5} = 4.2\n",
    "$$\n",
    "\n",
    "- Gene 3 mean:\n",
    "\n",
    "$$\n",
    "\\bar{x}_3 = \\frac{5 + 8 + 2 + 10 + 6}{5} = 6.2\n",
    "$$\n",
    "\n",
    "Center the matrix:\n",
    "\n",
    "$$\n",
    "\\tilde{X}_k = X_k - \\text{mean}(X_k)\n",
    "$$\n",
    "\n",
    "Resulting in the centered matrix:\n",
    "\n",
    "$$\n",
    "\\tilde{X}_k = \\begin{pmatrix}\n",
    "-1 & -1.2 & -1.2 \\\\\n",
    "1 & 1.8 & 1.8 \\\\\n",
    "-2 & -3.2 & -4.2 \\\\\n",
    "2 & 2.8 & 3.8 \\\\\n",
    "0 & -0.2 & -0.2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Step 2: Calculate Covariance Matrix\n",
    "\n",
    "The covariance matrix is defined as:\n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{n-1} \\tilde{X}_k^\\top \\tilde{X}_k\n",
    "$$\n",
    "\n",
    "where $n = 5$, so divide by 4.\n",
    "\n",
    "First, calculate $\\tilde{X}_k^\\top \\tilde{X}_k$:\n",
    "\n",
    "- (1,1) element:\n",
    "\n",
    "$$\n",
    "(-1)^2 + (1)^2 + (-2)^2 + (2)^2 + (0)^2 = 10\n",
    "$$\n",
    "\n",
    "- (1,2) element:\n",
    "\n",
    "$$\n",
    "(-1)(-1.2) + (1)(1.8) + (-2)(-3.2) + (2)(2.8) + (0)(-0.2) = 15\n",
    "$$\n",
    "\n",
    "- (1,3) element:\n",
    "\n",
    "$$\n",
    "(-1)(-1.2) + (1)(1.8) + (-2)(-4.2) + (2)(3.8) + (0)(-0.2) = 19\n",
    "$$\n",
    "\n",
    "- (2,2) element:\n",
    "\n",
    "$$\n",
    "(-1.2)^2 + (1.8)^2 + (-3.2)^2 + (2.8)^2 + (-0.2)^2 = 22.8\n",
    "$$\n",
    "\n",
    "- (2,3) element:\n",
    "\n",
    "$$\n",
    "(-1.2)(-1.2) + (1.8)(1.8) + (-3.2)(-4.2) + (2.8)(3.8) + (-0.2)(-0.2) = 28.8\n",
    "$$\n",
    "\n",
    "- (3,3) element:\n",
    "\n",
    "$$\n",
    "(-1.2)^2 + (1.8)^2 + (-4.2)^2 + (3.8)^2 + (-0.2)^2 = 36.8\n",
    "$$\n",
    "\n",
    "Since the covariance matrix is symmetric, fill in accordingly:\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\tilde{X}_k^\\top \\tilde{X}_k = \\begin{pmatrix}\n",
    "10 & 15 & 19 \\\\\n",
    "15 & 22.8 & 28.8 \\\\\n",
    "19 & 28.8 & 36.8 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Divide by 4 to get the covariance matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\begin{pmatrix}\n",
    "2.5 & 3.75 & 4.75 \\\\\n",
    "3.75 & 5.7 & 7.2 \\\\\n",
    "4.75 & 7.2 & 9.2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Step 3: Find the First Principal Component\n",
    "\n",
    "Now find the largest eigenvalue $\\lambda_1$ and corresponding eigenvector $v_1$ of $\\Sigma_k$.\n",
    "\n",
    "(**Details omitted**; usually solved with numerical software like Python.)\n",
    "\n",
    "Approximate normalized first eigenvector:\n",
    "\n",
    "$$\n",
    "v_1 \\approx \\begin{pmatrix}\n",
    "0.39 \\\\\n",
    "0.59 \\\\\n",
    "0.71 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Step 4: Calculate Module Eigengene $\\text{ME}_k$\n",
    "\n",
    "Directly left-multiply:\n",
    "\n",
    "$$\n",
    "\\text{ME}_k = \\tilde{X}_k v_1\n",
    "$$\n",
    "\n",
    "Compute the dot product row by row to get the $\\text{ME}_k$ values for 5 samples. For example:\n",
    "\n",
    "- Sample 1:\n",
    "\n",
    "$$\n",
    "(-1)(0.39) + (-1.2)(0.59) + (-1.2)(0.71) \\approx -1.95\n",
    "$$\n",
    "\n",
    "(and so forth for each sample)\n",
    "\n",
    "### ðŸŽ¯ Why Must PCA Be Centered? (Detailed Explanation)\n",
    "\n",
    "#### Objective of PCA\n",
    "\n",
    "Principal Component Analysis (PCA) seeks to find a direction $v$ such that the variance of the data projected onto $v$ is maximized:\n",
    "\n",
    "$$\n",
    "v_1 = \\arg\\max_{\\|v\\| = 1} \\operatorname{Var}(Xv)\n",
    "$$\n",
    "\n",
    "#### How Is Variance Defined?\n",
    "\n",
    "Standard sample variance:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(x) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\n",
    "$$\n",
    "\n",
    "Note: the mean $\\bar{x}$ must be subtractedâ€”**centering** is necessary.\n",
    "\n",
    "### What If We Don't Center?\n",
    "\n",
    "Using raw data to calculate:\n",
    "\n",
    "$$\n",
    "X^\\top X\n",
    "$$\n",
    "\n",
    "would include information about the mean shift. For example, if all samples are shifted up/right, the covariance would artificially inflate, even though the true relationships among variables remain unchanged.\n",
    "\n",
    "Thus, **uncentered PCA** could mistakenly find directions related to \"overall data drift\" rather than \"maximum variation.\" \n",
    "\n",
    "#### Correct Procedure âœ…\n",
    "\n",
    "First center each column (subtract its mean):\n",
    "\n",
    "$$\n",
    "\\tilde{X} = X - \\bar{X}\n",
    "$$\n",
    "\n",
    "Then compute the covariance matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n-1} \\tilde{X}^\\top \\tilde{X}\n",
    "$$\n",
    "\n",
    "Eigen-decompose $\\Sigma$ to find the true direction $v_1$ of maximum variance.\n",
    "\n",
    "### Why Specifically the First Principal Component $v_1$?\n",
    "\n",
    "#### 1. **Initial Motivation**\n",
    "\n",
    "Remember our goal:\n",
    "\n",
    "- Module $M_k$ has dozens or hundreds of genes\n",
    "- Each sample has a high-dimensional gene expression profile\n",
    "- Directly analyzing these is **high-dimensional, redundant, and noisy**, making downstream analysis difficult.\n",
    "\n",
    "We want to:\n",
    "\n",
    "âœ¨ **Summarize module expression into a single, simple value!**\n",
    "\n",
    "#### 2. **Why the First Principal Component?**\n",
    "\n",
    "Because the first principal component has these powerful mathematical properties:\n",
    "\n",
    "| Property | Description |\n",
    "|:--|:--|\n",
    "| Maximum Variance | It captures the direction along which the module's samples have the greatest spread. |\n",
    "| Most Information Preserved | It retains the most information among all possible 1D summaries. |\n",
    "| Noise Reduction | It focuses on the main trend and filters out minor noise. |\n",
    "\n",
    "In short:\n",
    "\n",
    "- First principal component = **maximum retention of the module's expression trend**!\n",
    "- No random or naive choice of direction retains more information.\n",
    "\n",
    "#### 3. **What If We Don't Use the First Principal Component?**\n",
    "\n",
    "- Random direction: projection variance is low; samples become indistinguishable.\n",
    "- Single gene: very vulnerable to noise or outliers.\n",
    "- Simple average: might obscure the dominant trend.\n",
    "\n",
    "The first principal component is **the only mathematically proven way** to:\n",
    "\n",
    "> **Maximally preserve information while reducing high-dimensional modules to a single vector.**\n",
    "\n",
    "#### Two Rigorous Mathematical Properties of the First Principal Component\n",
    "\n",
    "- Maximum explained variance\n",
    "- Minimum reconstruction error in low dimension\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Module-Trait Association\n",
    "\n",
    "Define the phenotype matrix:\n",
    "\n",
    "$$\n",
    "Y \\in \\mathbb{R}^{n \\times q}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$: number of samples\n",
    "- $q$: number of traits\n",
    "- The $j$-th trait is denoted $Y_{\\cdot j} \\in \\mathbb{R}^n$\n",
    "\n",
    "For each module $k$, with module eigengene:\n",
    "\n",
    "$$\n",
    "\\text{ME}_k \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "Define the correlation between module $k$ and trait $j$ as:\n",
    "\n",
    "$$\n",
    "r_{kj} = \\text{cor}(\\text{ME}_k, Y_{\\cdot j})\n",
    "$$\n",
    "\n",
    "using Pearson correlation.\n",
    "\n",
    "### Significance Testing\n",
    "\n",
    "To determine whether correlations are significant:\n",
    "\n",
    "- Null hypothesis: no relationship, $H_0: r_{kj} = 0$\n",
    "- Alternative hypothesis: significant linear relationship, $H_1: r_{kj} \\neq 0$\n",
    "\n",
    "Compute the $t$-statistic:\n",
    "\n",
    "$$\n",
    "t = \\frac{r_{kj} \\cdot \\sqrt{n - 2}}{\\sqrt{1 - r_{kj}^2}}\n",
    "$$\n",
    "\n",
    "with degrees of freedom:\n",
    "\n",
    "$$\n",
    "\\text{df} = n - 2\n",
    "$$\n",
    "\n",
    "thus obtaining a p-value.\n",
    "\n",
    "### Multiple Testing Correction\n",
    "\n",
    "Since multiple modules and multiple traits are tested, a total of $K \\times q$ tests are conducted. False positive rates must be controlled.\n",
    "\n",
    "Common methods:\n",
    "- Benjaminiâ€“Hochberg FDR control\n",
    "- Bonferroni correction (more conservative)\n",
    "\n",
    "After adjusting p-values, significant module-trait relationships can be selected.\n",
    "\n",
    "---\n",
    "\n",
    "# Overall Workflow Summary\n",
    "\n",
    "1. Input expression matrix $X$\n",
    "2. Compute gene similarity matrix $S$\n",
    "3. Construct adjacency matrix $A$ via soft-thresholding\n",
    "4. Compute topological overlap matrix $T$\n",
    "5. Cluster based on $1-T$ to identify modules $M_k$\n",
    "6. Extract module eigengenes $\\text{ME}_k$\n",
    "7. Correlate modules with traits $Y$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
